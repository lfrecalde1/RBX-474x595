{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593eb79b",
   "metadata": {},
   "source": [
    "# P1: Nifty Neural Networks!\n",
    "\n",
    "\n",
    "## Table Of Content\n",
    "\n",
    "1. Introduction\n",
    "2. Preliminaries\n",
    "3. Software Setup\n",
    "4. Implementation\n",
    "5. Grading Rubric\n",
    "6. Report guidelines\n",
    "7. Useful Resources\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Neural networks, at their core, function like any other mathematical function that can be evaluated. The process of evaluating a neural network is referred to as the forward pass. During this step, inputs are passed through the network layers, and outputs are generated.\n",
    "\n",
    "To optimize the network's performance, its weights and biases need to be adjusted. This is done through a process called backward propagation (or backpropagation). In this step, the gradients of the loss function with respect to each parameter are calculated, and these gradients are subtracted from the corresponding weights and biases, allowing the network to learn and improve its predictions.\n",
    "\n",
    "In this assignment, you will dive into the implementation of custom layers in PyTorch. Specifically, you will focus on coding the forward pass and computing the gradients necessary for the backward pass. Before you begin, make sure to review the grading rubric to understand the criteria for evaluation.\n",
    "\n",
    "## 2. Preliminaries\n",
    "\n",
    "### CIFAR10 Dataset\n",
    "\n",
    "CIFAR-10 is a dataset consisting of 60000, 32Ã—32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More details about the datset can be found [here](http://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Sample images from each class of the CIFAR-10 dataset is shown below:\n",
    "\n",
    "![CIFAR 10](./artifacts/cifar10.png)\n",
    "\n",
    "In this project, you will classify images into these 10 classes using the provided pipeline,loaders and helper classes.\n",
    "\n",
    "Additionally, you are expected to generate a confusion matrix to evaluate your model's performance. For guidance on plotting a confusion matrix in PyTorch, please refer to this [resource](https://stackoverflow.com/questions/74020233/how-to-plot-confusion-matrix-in-pytorch).\n",
    "\n",
    "### Linear Layer\n",
    "A linear layer in a neural network performs a linear transformation of the input data. It is defined by the following components:\n",
    "\n",
    "1. Weights\n",
    "2. Biases\n",
    "\n",
    "More details below,\n",
    "\n",
    "[Pytorch Linear Layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "You can find information about the dimension of weights and biases in custom_layers.py\n",
    "\n",
    "### Soft Max\n",
    "The Softmax function is commonly used in neural networks for multi-class classification problems. It converts a vector of raw scores (logits) into probabilities, making it possible to interpret the output as the likelihood of each class.\n",
    "\n",
    "[Sample implementation](https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python)\n",
    "\n",
    "More details [here](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "A convolutional layer is a fundamental building block in Convolutional Neural Networks (CNNs) used primarily for processing grid-like data such as images. It applies convolution operations to detect local features in the input.\n",
    "\n",
    "Although it is called a convolutional layer, the PyTorch implementation of conv2d does not actually perform a convolution in the mathematical sense. Instead, it performs a cross-correlation operation, where the kernel is not flipped. This distinction is important to note, but for most deep learning projects including this one, cross-correlation is perfectly fine as the weights will automatically adjust during training.\n",
    "\n",
    "For more details, refer to [P0](https://rbe549.github.io/rbe474x/fall2024a/proj/p0/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f305e",
   "metadata": {},
   "source": [
    "## 3. Software Setup\n",
    "\n",
    "Use a code editor like VSCode and open this entire folder.\n",
    "\n",
    "For each part, you will be implementing the corresponding layers in custom_layers.py\n",
    "\n",
    "The code will automatically be tested with test.py. \n",
    "\n",
    "To run the test, open a terminal in the current folder and run,\n",
    "\n",
    "`pytest -s -v test.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c586079",
   "metadata": {},
   "source": [
    "## 4. Implementation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925fff4",
   "metadata": {},
   "source": [
    "### Part1 : Implement Your Custom Layers for Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb96b9",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement a fully connected, relu and softmax layer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc90238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fer/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear\n",
      "Inference for linear layer\n",
      "tensor([[ 0.1505, -0.3249]], grad_fn=<CustomLinearLayerBackward>)\n",
      "tensor([[ 0.1505, -0.3249]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "graidents for linear layer\n",
      "tensor([[ 0.0499,  0.0237,  0.1034,  0.0421,  0.0830,  0.0179,  0.1026,  0.1116,\n",
      "          0.0253,  0.1190],\n",
      "        [-0.1077, -0.0511, -0.2233, -0.0909, -0.1791, -0.0387, -0.2216, -0.2411,\n",
      "         -0.0546, -0.2569]])\n",
      "tensor([[ 0.0499,  0.0237,  0.1034,  0.0421,  0.0830,  0.0179,  0.1026,  0.1116,\n",
      "          0.0253,  0.1190],\n",
      "        [-0.1077, -0.0511, -0.2233, -0.0909, -0.1791, -0.0387, -0.2216, -0.2411,\n",
      "         -0.0546, -0.2569]])\n",
      "tensor([ 0.1505, -0.3249])\n",
      "tensor([ 0.1505, -0.3249])\n",
      "tensor([[ 0.0819,  0.0190, -0.0509,  0.0489,  0.0473,  0.0880,  0.0001,  0.0627,\n",
      "         -0.0557,  0.0576]])\n",
      "tensor([[ 0.0819,  0.0190, -0.0509,  0.0489,  0.0473,  0.0880,  0.0001,  0.0627,\n",
      "         -0.0557,  0.0576]])\n",
      "\n",
      "RELU\n",
      "inference\n",
      "tensor([[0.5877, 0.2480, 0.7755, 0.3367, 0.5868, 0.5700, 0.9060, 0.5867, 0.3415,\n",
      "         0.9068]], grad_fn=<CustomReLULayerBackward>) tensor([[0.5877, 0.2480, 0.7755, 0.3367, 0.5868, 0.5700, 0.9060, 0.5867, 0.3415,\n",
      "         0.9068]], grad_fn=<ReluBackward0>)\n",
      "gradients of loss relative to the input\n",
      "tensor([[0.1175, 0.0496, 0.1551, 0.0673, 0.1174, 0.1140, 0.1812, 0.1173, 0.0683,\n",
      "         0.1814]])\n",
      "tensor([[0.1175, 0.0496, 0.1551, 0.0673, 0.1174, 0.1140, 0.1812, 0.1173, 0.0683,\n",
      "         0.1814]])\n",
      "\n",
      " SoftMax\n",
      "tensor([[0.1272, 0.0821, 0.1382, 0.1180, 0.1066, 0.1223, 0.0573, 0.0877, 0.0992,\n",
      "         0.0614]], grad_fn=<CustomSoftmaxLayerBackward>)\n",
      "tensor([[0.1272, 0.0821, 0.1382, 0.1180, 0.1066, 0.1223, 0.0573, 0.0877, 0.0992,\n",
      "         0.0614]], grad_fn=<SoftmaxBackward0>)\n",
      "gradients of loss relative to the input\n",
      "tensor([[ 5.1721e-04, -4.0668e-04,  8.6536e-04,  2.6408e-04, -5.0519e-06,\n",
      "          3.7871e-04, -5.6790e-04, -3.3588e-04, -1.5162e-04, -5.5824e-04]])\n",
      "tensor([[ 5.1721e-04, -4.0668e-04,  8.6536e-04,  2.6408e-04, -5.0519e-06,\n",
      "          3.7871e-04, -5.6790e-04, -3.3588e-04, -1.5162e-04, -5.5824e-04]])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import networks as net\n",
    "importlib.reload(net)\n",
    "\n",
    "print(\"\\nLinear\")\n",
    "\n",
    "# Check gradient input \n",
    "u_custom  = torch.rand((1, 10), requires_grad=True)  # leaf with grad\n",
    "u_inbuilt = u_custom.detach().clone().requires_grad_(True)\n",
    "\n",
    "customLayer = net.CustomLinear(10, 2)\n",
    "inbuiltLayer = nn.Linear(in_features=10, out_features=2)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "y_custom = customLayer(u_custom)\n",
    "y_inbuilt = inbuiltLayer(u_inbuilt)\n",
    "\n",
    "print(\"Inference for linear layer\")\n",
    "print(y_custom)\n",
    "print(y_inbuilt)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"\\ngraidents for linear layer\")\n",
    "print(customLayer.weight.grad)\n",
    "print(inbuiltLayer.weight.grad)\n",
    "\n",
    "print(customLayer.bias.grad)\n",
    "print(inbuiltLayer.bias.grad)\n",
    "\n",
    "print(u_custom.grad)\n",
    "print(u_inbuilt.grad)\n",
    "\n",
    "# RELU\n",
    "print(\"\\nRELU\")\n",
    "u1 = torch.rand((1, 10), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "customLayer = net.CustomReLU()\n",
    "inbuiltLayer = nn.ReLU()\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"inference\")\n",
    "print(y_custom, y_inbuilt)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n",
    "\n",
    "# SOFTMAX\n",
    "print(\"\\n SoftMax\")\n",
    "\n",
    "u1 = torch.rand((1, 10), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "customLayer = net.CustomSoftmax(1)\n",
    "inbuiltLayer = nn.Softmax(-1)\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "print(y_custom)\n",
    "print(y_inbuilt)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0ae69",
   "metadata": {},
   "source": [
    "### Part 2: MLP Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d7c7",
   "metadata": {},
   "source": [
    "Now that you have implemented an MLP from scratch, it's time to train it and verify its ability to classify objects. This network is expected to achieve an accuracy of approximately 40%.\n",
    "\n",
    "Additionally, you are required to save one of your best model checkpoints as mlp.pth in the current folder. This file will be used for automated testing.\n",
    "\n",
    "Furthermore, please implement a confusion matrix in the utils file, specifically within the val_step method of the Pipeline class. You may use any available implementation of the confusion matrix, but ensure that all tests continue to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8da3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch count:  0\n",
      "0 3400.105337023735 31.75\n",
      "BEST saved mlp.pth at epoch 0 (val_acc=31.75%)\n",
      "Epoch count:  1\n",
      "1 3328.2176662683487 33.69\n",
      "BEST saved mlp.pth at epoch 1 (val_acc=33.69%)\n",
      "Epoch count:  2\n",
      "2 3290.448949098587 35.49\n",
      "BEST saved mlp.pth at epoch 2 (val_acc=35.49%)\n",
      "Epoch count:  3\n",
      "3 3259.4535195827484 36.82\n",
      "BEST saved mlp.pth at epoch 3 (val_acc=36.82%)\n",
      "Epoch count:  4\n",
      "4 3234.9716527462006 36.85\n",
      "BEST saved mlp.pth at epoch 4 (val_acc=36.85%)\n",
      "Epoch count:  5\n",
      "5 3211.610500097275 38.58\n",
      "BEST saved mlp.pth at epoch 5 (val_acc=38.58%)\n",
      "Epoch count:  6\n",
      "6 3188.7219336032867 39.36\n",
      "BEST saved mlp.pth at epoch 6 (val_acc=39.36%)\n",
      "Epoch count:  7\n",
      "7 3166.2890664339066 39.56\n",
      "BEST saved mlp.pth at epoch 7 (val_acc=39.56%)\n",
      "Epoch count:  8\n",
      "8 3147.1115021705627 40.53\n",
      "BEST saved mlp.pth at epoch 8 (val_acc=40.53%)\n",
      "Epoch count:  9\n",
      "9 3128.866179704666 41.19\n",
      "BEST saved mlp.pth at epoch 9 (val_acc=41.19%)\n",
      "Epoch count:  10\n",
      "10 3111.657321691513 41.86\n",
      "BEST saved mlp.pth at epoch 10 (val_acc=41.86%)\n",
      "Epoch count:  11\n",
      "11 3096.574968099594 42.23\n",
      "BEST saved mlp.pth at epoch 11 (val_acc=42.23%)\n",
      "Epoch count:  12\n",
      "12 3081.9851573705673 42.76\n",
      "BEST saved mlp.pth at epoch 12 (val_acc=42.76%)\n",
      "Epoch count:  13\n",
      "13 3067.6454503536224 42.24\n",
      "Epoch count:  14\n",
      "14 3054.6034014225006 42.94\n",
      "BEST saved mlp.pth at epoch 14 (val_acc=42.94%)\n",
      "Epoch count:  15\n",
      "15 3041.138503074646 42.72\n",
      "Epoch count:  16\n",
      "16 3027.8739134073257 43.36\n",
      "BEST saved mlp.pth at epoch 16 (val_acc=43.36%)\n",
      "Epoch count:  17\n",
      "17 3016.3062435388565 42.64\n",
      "Epoch count:  18\n",
      "18 3005.473079800606 43.15\n",
      "Epoch count:  19\n",
      "19 2993.3759167194366 43.46\n",
      "BEST saved mlp.pth at epoch 19 (val_acc=43.46%)\n",
      "Epoch count:  20\n",
      "20 2983.1856096982956 43.64\n",
      "BEST saved mlp.pth at epoch 20 (val_acc=43.64%)\n",
      "Epoch count:  21\n",
      "21 2973.163915514946 43.15\n",
      "Epoch count:  22\n",
      "22 2963.7238091230392 43.45\n",
      "Epoch count:  23\n",
      "23 2953.3407530784607 43.83\n",
      "BEST saved mlp.pth at epoch 23 (val_acc=43.83%)\n",
      "Epoch count:  24\n",
      "24 2943.784289240837 43.67\n",
      "Epoch count:  25\n",
      "25 2933.890713453293 43.52\n",
      "Epoch count:  26\n",
      "26 2927.2923818826675 43.73\n",
      "Epoch count:  27\n",
      "27 2917.759468317032 44.08\n",
      "BEST saved mlp.pth at epoch 27 (val_acc=44.08%)\n",
      "Epoch count:  28\n",
      "28 2909.617181777954 43.61\n",
      "Epoch count:  29\n",
      "29 2901.896131157875 44.31\n",
      "BEST saved mlp.pth at epoch 29 (val_acc=44.31%)\n",
      "Epoch count:  30\n",
      "30 2895.511975288391 44.23\n",
      "Epoch count:  31\n",
      "31 2887.3923048973083 43.75\n",
      "Epoch count:  32\n",
      "32 2880.5448479652405 43.96\n",
      "Epoch count:  33\n",
      "33 2872.8662720918655 43.81\n",
      "Epoch count:  34\n",
      "34 2866.898262858391 44.27\n",
      "Epoch count:  35\n",
      "35 2861.4058653116226 44.16\n",
      "Epoch count:  36\n",
      "36 2854.0470749139786 44.1\n",
      "Epoch count:  37\n",
      "37 2848.1363822221756 44.09\n",
      "Epoch count:  38\n",
      "38 2843.62861764431 44.6\n",
      "BEST saved mlp.pth at epoch 38 (val_acc=44.60%)\n",
      "Epoch count:  39\n",
      "39 2837.4107632637024 44.36\n"
     ]
    }
   ],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "#importlib.reload(net)\n",
    "pipeline = net.Pipeline()\n",
    "model = net.CustomMLP().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"./\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/mlp/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "\n",
    "best_val_acc = -float(\"inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "\n",
    "for eIndex in range(epochs):\n",
    "    print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = eIndex\n",
    "        torch.save(model.state_dict(), \"mlp.pth\") \n",
    "        print(f\"BEST saved mlp.pth at epoch {eIndex} (val_acc={val_acc:.2f}%)\")\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train_mlp.log\", trainLosses)\n",
    "np.savetxt(\"test_mlp.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0058c51-8904-43a5-a0c6-8dba6723c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "def plot_samples(data, label, name):\n",
    "    with plt.style.context([\"science\", \"no-latex\"]):\n",
    "        fig, ax = plt.subplots(figsize=(8, 2))\n",
    "\n",
    "        \n",
    "        ax.plot(data, label=label)\n",
    "\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(label)\n",
    "        ax.autoscale(tight=True)\n",
    "        ax.legend()\n",
    "\n",
    "        \n",
    "        fig.savefig(f\"{name}.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6a003c-fbdd-4e8a-bf6f-864cdc2b3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(trainLosses, \"Loss\", \"MLP_LOSS\")\n",
    "plot_samples(testAccuracies, \"Accuracy \", \"MLP_ACCURACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b1e2d",
   "metadata": {},
   "source": [
    "### Part 3: Implement Convolutional Neural Networks (CNN) Using PyTorch layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c6b38",
   "metadata": {},
   "source": [
    "CNNs excel in capturing local patterns and spatial hierarchies through convolutional filters, which makes them more effective for image and spatial data. They also use parameter sharing, reducing the number of parameters and computational cost compared to MLPs. Additionally, CNNs offer translation invariance and hierarchical feature learning, enabling them to recognize features across different spatial locations and build complex patterns efficiently.\n",
    "\n",
    "Open networks.py and implement `RefCNN` using the inbuilt layers in pytorch. Make sure it is similar to CustomCNN() which uses custom layers.\n",
    "\n",
    "Train and compare the train loss and validation accuracy against MLP. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as cnn_inbuilt.pth for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9a9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train complete\n",
      "0 3396.3351991176605 31.97\n",
      "BEST saved cnn_inbuilt at epoch 0 (val_acc=31.97%)\n",
      "train complete\n",
      "1 3316.9577473402023 35.02\n",
      "BEST saved cnn_inbuilt at epoch 1 (val_acc=35.02%)\n",
      "train complete\n",
      "2 3275.3401931524277 37.6\n",
      "BEST saved cnn_inbuilt at epoch 2 (val_acc=37.60%)\n",
      "train complete\n",
      "3 3237.8501485586166 40.4\n",
      "BEST saved cnn_inbuilt at epoch 3 (val_acc=40.40%)\n",
      "train complete\n",
      "4 3199.2420811653137 41.23\n",
      "BEST saved cnn_inbuilt at epoch 4 (val_acc=41.23%)\n",
      "train complete\n",
      "5 3162.697119116783 43.45\n",
      "BEST saved cnn_inbuilt at epoch 5 (val_acc=43.45%)\n",
      "train complete\n",
      "6 3130.3807402849197 45.09\n",
      "BEST saved cnn_inbuilt at epoch 6 (val_acc=45.09%)\n",
      "train complete\n",
      "7 3104.2013511657715 46.01\n",
      "BEST saved cnn_inbuilt at epoch 7 (val_acc=46.01%)\n",
      "train complete\n",
      "8 3080.251173377037 45.23\n",
      "train complete\n",
      "9 3057.188807249069 46.52\n",
      "BEST saved cnn_inbuilt at epoch 9 (val_acc=46.52%)\n",
      "train complete\n",
      "10 3038.0935193300247 47.13\n",
      "BEST saved cnn_inbuilt at epoch 10 (val_acc=47.13%)\n",
      "train complete\n",
      "11 3016.2621853351593 47.22\n",
      "BEST saved cnn_inbuilt at epoch 11 (val_acc=47.22%)\n",
      "train complete\n",
      "12 2996.579768180847 49.12\n",
      "BEST saved cnn_inbuilt at epoch 12 (val_acc=49.12%)\n",
      "train complete\n",
      "13 2977.5164157152176 48.96\n",
      "train complete\n",
      "14 2958.235035777092 50.0\n",
      "BEST saved cnn_inbuilt at epoch 14 (val_acc=50.00%)\n",
      "train complete\n",
      "15 2940.726924777031 50.71\n",
      "BEST saved cnn_inbuilt at epoch 15 (val_acc=50.71%)\n",
      "train complete\n",
      "16 2921.5922082662582 50.41\n",
      "train complete\n",
      "17 2903.356253385544 50.85\n",
      "BEST saved cnn_inbuilt at epoch 17 (val_acc=50.85%)\n",
      "train complete\n",
      "18 2886.281483411789 51.37\n",
      "BEST saved cnn_inbuilt at epoch 18 (val_acc=51.37%)\n",
      "train complete\n",
      "19 2868.9107090234756 51.78\n",
      "BEST saved cnn_inbuilt at epoch 19 (val_acc=51.78%)\n",
      "train complete\n",
      "20 2852.1031688451767 52.03\n",
      "BEST saved cnn_inbuilt at epoch 20 (val_acc=52.03%)\n",
      "train complete\n",
      "21 2835.606729745865 52.31\n",
      "BEST saved cnn_inbuilt at epoch 21 (val_acc=52.31%)\n",
      "train complete\n",
      "22 2819.014874815941 52.36\n",
      "BEST saved cnn_inbuilt at epoch 22 (val_acc=52.36%)\n",
      "train complete\n",
      "23 2805.3785716295242 52.44\n",
      "BEST saved cnn_inbuilt at epoch 23 (val_acc=52.44%)\n",
      "train complete\n",
      "24 2789.3406676054 52.22\n",
      "train complete\n",
      "25 2775.034240961075 52.77\n",
      "BEST saved cnn_inbuilt at epoch 25 (val_acc=52.77%)\n",
      "train complete\n",
      "26 2762.287117242813 52.93\n",
      "BEST saved cnn_inbuilt at epoch 26 (val_acc=52.93%)\n",
      "train complete\n",
      "27 2750.233090400696 52.97\n",
      "BEST saved cnn_inbuilt at epoch 27 (val_acc=52.97%)\n",
      "train complete\n",
      "28 2736.8123939037323 53.27\n",
      "BEST saved cnn_inbuilt at epoch 28 (val_acc=53.27%)\n",
      "train complete\n",
      "29 2725.4939209222794 53.21\n",
      "train complete\n",
      "30 2715.8515790700912 53.56\n",
      "BEST saved cnn_inbuilt at epoch 30 (val_acc=53.56%)\n",
      "train complete\n",
      "31 2705.8298958539963 53.76\n",
      "BEST saved cnn_inbuilt at epoch 31 (val_acc=53.76%)\n",
      "train complete\n",
      "32 2696.152527451515 53.82\n",
      "BEST saved cnn_inbuilt at epoch 32 (val_acc=53.82%)\n",
      "train complete\n",
      "33 2688.1950002908707 53.56\n",
      "train complete\n",
      "34 2680.483283162117 53.21\n",
      "train complete\n",
      "35 2672.0206404924393 53.98\n",
      "BEST saved cnn_inbuilt at epoch 35 (val_acc=53.98%)\n",
      "train complete\n",
      "36 2664.672456741333 53.76\n",
      "train complete\n",
      "37 2659.8493645191193 53.76\n",
      "train complete\n",
      "38 2653.1488468647003 53.85\n",
      "train complete\n",
      "39 2647.2071439027786 53.58\n"
     ]
    }
   ],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.RefCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_inbuilt_layers/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "\n",
    "best_val_acc = -float(\"inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = eIndex\n",
    "        torch.save(model.state_dict(), \"cnn_inbuilt.pth\") \n",
    "        print(f\"BEST saved cnn_inbuilt at epoch {eIndex} (val_acc={val_acc:.2f}%)\")\n",
    "    \n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train_cnn_inbuilt.log\", trainLosses)\n",
    "np.savetxt(\"test_cnn_inbuilt.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4065ab3-1acd-49b1-baa9-1439ce76c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(trainLosses, \"Loss\", \"CNN_INBUILT_LOSS\")\n",
    "plot_samples(testAccuracies, \"Accuracy \", \"CNN_INBUILT_ACCURACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b9224",
   "metadata": {},
   "source": [
    "### Part 4: Implement Your Custom Layers for Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78b4fe",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement the CustomConvLayer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7dd42-15e1-4f71-b243-5b60004d4ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23dc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv. Inference\n",
      "tensor([[[[-0.2506, -2.8219],\n",
      "          [-0.5303, -1.0692]],\n",
      "\n",
      "         [[ 2.3159,  1.5788],\n",
      "          [ 3.5786,  3.0594]],\n",
      "\n",
      "         [[ 1.4651,  0.8561],\n",
      "          [-0.8348,  0.7310]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[-0.2506, -2.8219],\n",
      "          [-0.5303, -1.0692]],\n",
      "\n",
      "         [[ 2.3159,  1.5788],\n",
      "          [ 3.5786,  3.0594]],\n",
      "\n",
      "         [[ 1.4651,  0.8561],\n",
      "          [-0.8348,  0.7310]]]], grad_fn=<CustomConvLayerBackward>)\n",
      "gradients of loss relative to the weights\n",
      "tensor([[[[-0.2154, -0.4804, -0.4178],\n",
      "          [-0.2466, -0.3324, -0.3091],\n",
      "          [-0.1791, -0.5252, -0.2178]],\n",
      "\n",
      "         [[-0.6315, -0.4024, -0.3032],\n",
      "          [-0.3305, -0.3392, -0.3301],\n",
      "          [-0.5373, -0.4513, -0.2497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5849,  0.8853,  0.5619],\n",
      "          [ 0.7895,  0.7153,  0.6262],\n",
      "          [ 0.5791,  1.2737,  0.3984]],\n",
      "\n",
      "         [[ 1.0028,  0.9618,  0.8793],\n",
      "          [ 0.7455,  0.9205,  0.7567],\n",
      "          [ 1.3350,  1.1468,  1.1105]]],\n",
      "\n",
      "\n",
      "        [[[-0.0353,  0.2340,  0.1807],\n",
      "          [ 0.0879,  0.0381,  0.0501],\n",
      "          [ 0.2049,  0.1660,  0.0683]],\n",
      "\n",
      "         [[ 0.1545,  0.1751,  0.2227],\n",
      "          [ 0.2460,  0.2131,  0.1453],\n",
      "          [ 0.2376,  0.2227,  0.1176]]]])\n",
      "tensor([[[[-0.2154, -0.4804, -0.4178],\n",
      "          [-0.2466, -0.3324, -0.3091],\n",
      "          [-0.1791, -0.5252, -0.2178]],\n",
      "\n",
      "         [[-0.6315, -0.4024, -0.3032],\n",
      "          [-0.3305, -0.3392, -0.3301],\n",
      "          [-0.5373, -0.4513, -0.2497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5849,  0.8853,  0.5619],\n",
      "          [ 0.7895,  0.7153,  0.6262],\n",
      "          [ 0.5791,  1.2737,  0.3984]],\n",
      "\n",
      "         [[ 1.0028,  0.9618,  0.8793],\n",
      "          [ 0.7455,  0.9205,  0.7567],\n",
      "          [ 1.3350,  1.1468,  1.1105]]],\n",
      "\n",
      "\n",
      "        [[[-0.0353,  0.2340,  0.1807],\n",
      "          [ 0.0879,  0.0381,  0.0501],\n",
      "          [ 0.2049,  0.1660,  0.0683]],\n",
      "\n",
      "         [[ 0.1545,  0.1751,  0.2227],\n",
      "          [ 0.2460,  0.2131,  0.1453],\n",
      "          [ 0.2376,  0.2227,  0.1176]]]])\n",
      "gradients of loss relative to the bias\n",
      "tensor([-0.7787,  1.7554,  0.3696])\n",
      "tensor([-0.7787,  1.7554,  0.3696])\n",
      "gradients of loss relative to the input\n",
      "tensor([[[[ 0.1865, -0.0017,  0.2201,  0.5777, -0.2279],\n",
      "          [-0.0948, -0.1177,  0.1163,  0.7651, -0.5520],\n",
      "          [ 0.8740,  0.1201,  0.7413, -0.2852,  0.3437],\n",
      "          [ 0.0125,  0.1287,  0.3266,  0.1798,  0.1916],\n",
      "          [ 0.0176,  0.1775,  0.9695, -0.2440,  0.3762]],\n",
      "\n",
      "         [[ 0.3972,  0.2492,  0.4744,  0.5132, -0.6717],\n",
      "          [ 1.0716,  0.5119,  0.7445,  0.7616,  0.1297],\n",
      "          [ 0.2326, -1.2353,  0.7593, -0.7465, -0.7121],\n",
      "          [ 0.3081,  0.1114,  0.9985,  0.5267,  0.1137],\n",
      "          [ 0.5967,  0.2586,  1.3630, -0.1779,  0.6928]]]])\n",
      "tensor([[[[ 0.1865, -0.0017,  0.2201,  0.5777, -0.2279],\n",
      "          [-0.0948, -0.1177,  0.1163,  0.7651, -0.5520],\n",
      "          [ 0.8740,  0.1201,  0.7413, -0.2852,  0.3437],\n",
      "          [ 0.0125,  0.1287,  0.3266,  0.1798,  0.1916],\n",
      "          [ 0.0176,  0.1775,  0.9695, -0.2440,  0.3762]],\n",
      "\n",
      "         [[ 0.3972,  0.2492,  0.4744,  0.5132, -0.6717],\n",
      "          [ 1.0716,  0.5119,  0.7445,  0.7616,  0.1297],\n",
      "          [ 0.2326, -1.2353,  0.7593, -0.7465, -0.7121],\n",
      "          [ 0.3081,  0.1114,  0.9985,  0.5267,  0.1137],\n",
      "          [ 0.5967,  0.2586,  1.3630, -0.1779,  0.6928]]]])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "inbuiltLayer = nn.Conv2d(2, 3, 3, stride=2, padding='valid')\n",
    "customLayer = net.CustomConv2d(2, 3, 3, 2)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "u1 = torch.rand((1, 2, 5, 5), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "y1 = inbuiltLayer(u1)\n",
    "y2 = customLayer(u2)\n",
    "\n",
    "print(\"Conv. Inference\")\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "loss_custom = lossFunc(y2, torch.zeros_like(y2))\n",
    "loss_in = lossFunc(y1, torch.zeros_like(y1))\n",
    "\n",
    "loss_in.backward()\n",
    "loss_custom.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the weights\")\n",
    "print(inbuiltLayer.weight.grad)\n",
    "print(customLayer.weight.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the bias\")\n",
    "print(inbuiltLayer.bias.grad)\n",
    "print(customLayer.bias.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf028b78",
   "metadata": {},
   "source": [
    "### Part 5: CNN Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca731",
   "metadata": {},
   "source": [
    "Train and compare the train loss and validation accuracy against MLP and inbuilt conv layers. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as `cnn_custom.pth` for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f31b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train complete\n",
      "0 3366.449597597122 35.06\n",
      "BEST saved cnn_custom at epoch 0 (val_acc=35.06%)\n",
      "train complete\n",
      "1 3253.6440790891647 39.99\n",
      "BEST saved cnn_custom at epoch 1 (val_acc=39.99%)\n",
      "train complete\n",
      "2 3190.0756834745407 41.37\n",
      "BEST saved cnn_custom at epoch 2 (val_acc=41.37%)\n",
      "train complete\n",
      "3 3137.132972240448 43.48\n",
      "BEST saved cnn_custom at epoch 3 (val_acc=43.48%)\n",
      "train complete\n",
      "4 3095.606787919998 44.77\n",
      "BEST saved cnn_custom at epoch 4 (val_acc=44.77%)\n",
      "train complete\n",
      "5 3055.4757573604584 46.08\n",
      "BEST saved cnn_custom at epoch 5 (val_acc=46.08%)\n",
      "train complete\n",
      "6 3018.9315123558044 46.16\n",
      "BEST saved cnn_custom at epoch 6 (val_acc=46.16%)\n",
      "train complete\n",
      "7 2988.6662899255753 48.22\n",
      "BEST saved cnn_custom at epoch 7 (val_acc=48.22%)\n",
      "train complete\n",
      "8 2956.8439984321594 48.4\n",
      "BEST saved cnn_custom at epoch 8 (val_acc=48.40%)\n",
      "train complete\n",
      "9 2927.659026145935 49.06\n",
      "BEST saved cnn_custom at epoch 9 (val_acc=49.06%)\n",
      "train complete\n",
      "10 2897.426343560219 50.08\n",
      "BEST saved cnn_custom at epoch 10 (val_acc=50.08%)\n",
      "train complete\n",
      "11 2869.7944333553314 50.09\n",
      "BEST saved cnn_custom at epoch 11 (val_acc=50.09%)\n",
      "train complete\n",
      "12 2839.3842005729675 50.71\n",
      "BEST saved cnn_custom at epoch 12 (val_acc=50.71%)\n",
      "train complete\n",
      "13 2811.624007463455 51.11\n",
      "BEST saved cnn_custom at epoch 13 (val_acc=51.11%)\n",
      "train complete\n",
      "14 2786.485918402672 51.21\n",
      "BEST saved cnn_custom at epoch 14 (val_acc=51.21%)\n",
      "train complete\n",
      "15 2762.2983096837997 51.65\n",
      "BEST saved cnn_custom at epoch 15 (val_acc=51.65%)\n",
      "train complete\n",
      "16 2740.8925145864487 51.7\n",
      "BEST saved cnn_custom at epoch 16 (val_acc=51.70%)\n",
      "train complete\n",
      "17 2719.9540407657623 52.49\n",
      "BEST saved cnn_custom at epoch 17 (val_acc=52.49%)\n",
      "train complete\n",
      "18 2701.4747409820557 51.85\n",
      "train complete\n",
      "19 2684.469920039177 52.05\n",
      "train complete\n",
      "20 2668.4916026592255 52.85\n",
      "BEST saved cnn_custom at epoch 20 (val_acc=52.85%)\n",
      "train complete\n",
      "21 2653.585173368454 52.47\n",
      "train complete\n",
      "22 2641.1785737276077 51.93\n",
      "train complete\n",
      "23 2628.291307091713 52.6\n",
      "train complete\n",
      "24 2617.1865986585617 52.93\n",
      "BEST saved cnn_custom at epoch 24 (val_acc=52.93%)\n",
      "train complete\n",
      "25 2608.376560330391 52.81\n",
      "train complete\n",
      "26 2599.18131005764 52.8\n",
      "train complete\n",
      "27 2590.4066091775894 53.32\n",
      "BEST saved cnn_custom at epoch 27 (val_acc=53.32%)\n",
      "train complete\n",
      "28 2583.147855758667 53.5\n",
      "BEST saved cnn_custom at epoch 28 (val_acc=53.50%)\n",
      "train complete\n",
      "29 2575.6117182970047 53.46\n",
      "train complete\n",
      "30 2571.2636590003967 53.34\n",
      "train complete\n",
      "31 2565.211534380913 52.83\n",
      "train complete\n",
      "32 2559.5299332141876 53.23\n",
      "train complete\n",
      "33 2555.8124109506607 52.64\n",
      "train complete\n",
      "34 2552.136097550392 53.59\n",
      "BEST saved cnn_custom at epoch 34 (val_acc=53.59%)\n",
      "train complete\n",
      "35 2547.7126554250717 52.64\n",
      "train complete\n",
      "36 2545.459522128105 52.45\n",
      "train complete\n",
      "37 2540.1940656900406 52.88\n",
      "train complete\n",
      "38 2536.993257164955 53.11\n",
      "train complete\n",
      "39 2534.853081345558 53.04\n"
     ]
    }
   ],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.CustomCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_custom_layer/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "\n",
    "best_val_acc = -float(\"inf\")\n",
    "best_epoch = -1\n",
    "\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = eIndex\n",
    "        torch.save(model.state_dict(), \"cnn_custom.pth\") \n",
    "        print(f\"BEST saved cnn_custom at epoch {eIndex} (val_acc={val_acc:.2f}%)\")\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train_cnn_custom.log\", trainLosses)\n",
    "np.savetxt(\"test_cnn_custom.log\", testAccuracies)\n",
    "\n",
    "plot_samples(trainLosses, \"Loss\", \"CNN_CUSTOM_LOSS\")\n",
    "plot_samples(testAccuracies, \"Accuracy \", \"CNN_CUSTOM_ACCURACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f90630ce-70df-4a0c-904a-d9677ab7fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative(data):\n",
    "    with plt.style.context([\"science\", \"no-latex\"]):\n",
    "        fig, ax = plt.subplots(figsize=(8, 2))\n",
    "        labels = [\"mlp\", \"cnn_inbuilt\", \"cnn_custom\"]\n",
    "        for i in range(data.shape[0]):\n",
    "            ax.plot(data[i, :], label=f\"{labels[i]}\")\n",
    "        ax.legend(loc = \"upper right\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.autoscale(tight=True)\n",
    "        fig.savefig(\"comparative.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7148c30c-127e-429e-8df5-32984a97b4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_custom = np.loadtxt(\"test_cnn_custom.log\")\n",
    "cnn_inbuilt = np.loadtxt(\"test_cnn_inbuilt.log\")\n",
    "mlp = np.loadtxt(\"test_mlp.log\")\n",
    "data = np.vstack((mlp, cnn_inbuilt, cnn_custom))\n",
    "plot_comparative(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272187",
   "metadata": {},
   "source": [
    "## 5. Grading Rubric\n",
    "\n",
    "- part 1 : 60\n",
    "- part 2 : 10\n",
    "- part 3 : 10\n",
    "- part 4 : 10\n",
    "- part 5 : 10\n",
    "\n",
    "For RBE474X: part1 + part2 + part3 = 100% of the grade (80/80).\n",
    "For RBE595-A01-SP: You are expected to implement part1-part5 for getting full credits (100/100).\n",
    "\n",
    "Your code will be evaluated with test.py. Please run it and ensure that the tests pass before submitting. Instructions are in software setup section.\n",
    "\n",
    "Please note that I will replace the test.py with my original test.py before evaluating.\n",
    "\n",
    "Please do not submit the data folder that is downloaded while training the network. It is over 300 MB. Anyone submitting data will be penalized! Your submission should not be more than 20 MB.\n",
    "\n",
    "## 6. Report Guidelines\n",
    "\n",
    "Report must be in Latex.\n",
    "\n",
    "Include the following,\n",
    "\n",
    "1. Training loss curve (loss vs epoch count)\n",
    "2. Confusion Matrix for validation set (val_step)\n",
    "3. Accuracy comparison between MLP, CNN (torch layers) and CNN (custom_layers)\n",
    "\n",
    "## 7. Useful Resources\n",
    "\n",
    "Here are some YouTube videos that can help you with project 1.\n",
    "\n",
    "1. [Back Propagation from scratch](https://www.youtube.com/watch?v=4shguqlkTDM)\n",
    "2. [Back Propagation](https://www.youtube.com/watch?v=YOlOLxrMUOw)\n",
    "3. [Back Propagation in CNN](https://www.youtube.com/watch?v=z9hJzduHToc)\n",
    "\n",
    "Additionally, you may find [torch einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html) function really helpful for multidimensional tensor-tensor products. It is a physics concept that got used/abused by programmers.\n",
    "\n",
    "[Index notation and torch einsum](https://www.youtube.com/watch?v=IvgV6QcsC64)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
